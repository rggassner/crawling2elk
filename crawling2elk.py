#!venv/bin/python3
import absl.logging
import argparse
import bs4.builder
import fcntl
import hashlib
import logging
import numpy as np
import os
import random
import re
import requests
import signal
import string
import subprocess
import time
import urllib3
import warnings
from bs4 import BeautifulSoup, MarkupResemblesLocatorWarning
from collections import Counter
from concurrent.futures import ThreadPoolExecutor, as_completed
from config import *
from datetime import datetime, timezone
from elasticsearch import helpers, ConflictError
from fake_useragent import UserAgent
from functions import *
from io import BytesIO
from pathlib import PurePosixPath
from PIL import Image, UnidentifiedImageError
from seleniumwire import webdriver
from seleniumwire.utils import decode
from tornado import httpserver, ioloop, web
from urllib.parse import unquote, urljoin, urlparse, urlsplit
from urllib3.exceptions import InsecureRequestWarning

if CATEGORIZE_NSFW:
    import opennsfw2 as n2
    model = n2.make_open_nsfw_model()

absl.logging.set_verbosity('error')
warnings.filterwarnings("ignore", category=InsecureRequestWarning)
warnings.filterwarnings(
        "ignore",
        category=Warning,
        message=".*verify_certs=False is insecure.*")
warnings.filterwarnings("ignore", category=MarkupResemblesLocatorWarning)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

url_functions = []
content_type_functions = []
lock_file = None  # Global reference to prevent garbage collection

# Used to generate wordlist
soup_tag_blocklist = [
    "[document]",
    "noscript",
    "header",
    "html",
    "meta",
    "head",
    "input",
    "script",
    "style",
]

## Verify if host is in a blocklist.
def is_host_block_listed(url):
    for regex in host_regex_block_list:
        if re.search(regex, url, flags=re.I | re.U):
            return True
    return False

## Verify if url is in a blocklist.
def is_url_block_listed(url):
    for regex in url_regex_block_list:
        if re.search(regex, url, flags=re.I | re.U):
            return True
    return False

## Verify if url is in a allowlist.
def is_host_allow_listed(url):
    for regex in host_regex_allow_list:
        if re.search(regex, url, flags=re.I | re.U):
            return True
    return False

def build_conditional_update_script(doc: dict) -> tuple[str, dict]:
    script_lines = []
    params = {}
    for key, value in doc.items():
        params[key] = value
        if key == "visited":
            script_lines.append("""
                if (ctx._source.visited == null || ctx._source.visited == false) {
                    ctx._source.visited = params.visited;
                }
            """)
        elif key != "updated_at":
            script_lines.append(f"""
                if (ctx._source['{key}'] != params['{key}']) {{
                    ctx._source['{key}'] = params['{key}'];
                }}
            """)
    # Only update updated_at if any other field changed
    script_lines.append("""
        if (ctx._source != params.existing_source_snapshot) {
            ctx._source.updated_at = params.updated_at;
        }
    """)

    return "\n".join(script_lines), params

def get_words(text: bytes | str) -> list[str]:
    if not text:
        return []
    if isinstance(text, bytes):
        try:
            text = text.decode('utf-8', errors='replace')
        except Exception:
            return []
    return extract_top_words_from_text(text)

def get_words_from_soup(soup) -> list[str]:
    text_parts = [
        t for t in soup.find_all(string=True)
        if t.parent.name not in soup_tag_blocklist
    ]
    combined_text = " ".join(text_parts)
    return extract_top_words_from_text(combined_text)

def get_min_webcontent(soup):
    text_parts = [
        t for t in soup.find_all(string=True)
        if t.parent.name not in soup_tag_blocklist
    ]
    combined_text = " ".join(text_parts)
    return combined_text

def extract_top_words_from_text(text: str) -> list[str]:
    if WORDS_REMOVE_SPECIAL_CHARS:
        text = re.sub(r'[^\w\s]', ' ', text, flags=re.UNICODE)
    if WORDS_TO_LOWER:
        text = text.lower()

    words = [word for word in text.split() if WORDS_MIN_LEN < len(word) <= WORDS_MAX_LEN]
    most_common = Counter(words).most_common(WORDS_MAX_WORDS)
    return [word for word, _ in most_common]

def is_open_directory(content, content_url):
    host = urlsplit(content_url)[1]
    hostnp = host.split(':')[0]

    patterns = [
        r'<title>Index of /',                                # Apache-style
        r'<h1>Index of /',                                   # Apache-style H1
        r'\[To Parent Directory\]</A>',                      # IIS-style
        r'<title>' + re.escape(host) + r' - /</title>',      # Lighttpd-style
        r'_sort=\'name\';SortDirsAndFilesName\(\)',          # h5ai
        r'<body[^>]*class="[^"]*dufs[^"]*"',                 # DUFS body
        r'<footer[^>]*>Generated by dufs',                   # DUFS footer
        r'<script[^>]*src="[^"]*dufs[^"]*"',                 # DUFS JS
        r'<div class="breadcrumbs">Folder Path</div>',       # Caddy-style breadcrumb
        r'<th><a href="\?C=N;O=D">Name</a></th><th><a href="\?C=M;O=A">Last modified</a></th><th><a href="\?C=S;O=A">Size</a></th><th><a href="\?C=D;O=A">Description</a></th>',
        r'<table class="sortable">\s*<thead>\s*<tr>\s*<th>Name\s*</th>\s*<th>Size\s*</th>\s*<th>Uploaded\s*</th>\s*<th>\s*</th>\s*</tr>',
        r'<title>Directory Listing</title>',
        r'<h1>Listing of /',
        r'Powered by <a class="autoindex_a" href="http://autoindex.sourceforge.net/">AutoIndex PHP Script</a>',
        r'<a href="\?C=N;O=D">\s*Name\s*</a>\s*<a href="\?C=M;O=A">\s*Last modified\s*</a>\s*<a href="\?C=S;O=A">\s*Size\s*</a>\s*<a href="\?C=D;O=A">\s*Description\s*</a>',
        r'<a href="\?C=N&amp;O=A">\s*File Name\s*</a>\s*&nbsp;\s*<a href="\?C=N&amp;O=D">\s*&nbsp;&darr;&nbsp;\s*</a></th>\s*<th style="width:20%">\s*<a href="\?C=S&amp;O=A">\s*File Size\s*</a>\s*&nbsp;\s*<a href="\?C=S&amp;O=D">\s*&nbsp;&darr;&nbsp;\s*</a>',
        r'<a href="\?C=N&amp;O=A">\s*File Name\s*</a>\s*(?:&nbsp;|\u00a0)\s*<a href="\?C=N&amp;O=D">\s*(?:&nbsp;|\u00a0)?(?:&darr;|\u2193)(?:&nbsp;|\u00a0)?\s*</a>[\s\S]*?<a href="\?C=S&amp;O=A">\s*File Size\s*</a>\s*(?:&nbsp;|\u00a0)\s*<a href="\?C=S&amp;O=D">\s*(?:&nbsp;|\u00a0)?(?:&darr;|\u2193)(?:&nbsp;|\u00a0)?\s*</a>',
        r'<meta\s+name="generator"\s+content="AList V\d+"\s*/?>',
        r'<meta\scontent="AList V\d+"\sname="generator"/?>',
        r'<div\s+id=["\']idx["\']>\s*<!--\s*do not remove\s*-->',
        r'<tr[^>]*class=["\']indexhead["\'][^>]*>.*Name.*Last modified.*Size.*Description',
        r'<pre>(?:\s*\d{1,2}/\d{1,2}/\d{4}\s+\d{1,2}:\d{2}\s+(?:AM|PM)?\s+\d+\s+<a href="[^"]+">[^<]+</a>\s*<br>\s*){2,}</pre>',
        r'<html><head><title>' + hostnp + r' - /[^<]*</title></head><body><h1>' + hostnp + r' - /[^<]*</h1>',
        r'<meta\s+name=["\']description["\']\s+content=["\']Yet another directory listing, powered by Directory Lister\.["\']\s*/?>',
        r'<meta\scontent="Yet\sanother\sdirectory\slisting,\spowered\sby\sDirectory\sLister\."\sname="description"/>',
        r'<title>\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\s*-\s*/</title>',
        r'<title>Index of .*?</title>',
        r'<h1>Index of .*?</h1>',
        r'<h1>文件索引.*?</h1>',
        r'Directory listing for .*',
        r'<ListBucketResult\s+xmlns=[\'\"].*?[\'\"]>',
        r'<tr\s+class=["\']indexhead["\']>\s*<th\s+class=["\']indexcolicon["\']>\s*<img\s+src=["\']/icons/blank\.gif["\']\s+alt=["\']\[ICO\]["\']\s*/?>\s*</th>\s*<th\s+class=["\']indexcolname["\']>\s*<a\s+href=["\']\?C=N;O=A["\']>\s*Name\s*</a>\s*</th>\s*<th\s+class=["\']indexcollastmod["\']>\s*<a\s+href=["\']\?C=M;O=A["\']>\s*Last\s+modified\s*</a>\s*</th>\s*<th\s+class=["\']indexcolsize["\']>\s*<a\s+href=["\']\?C=S;O=A["\']>\s*Size\s*</a>\s*</th>\s*</tr>',
        r'\.calibreRangeWrapper',
        r'<body\sstyle="font-size:medium">[a-z]*\sFolder\s*\t*<a\shref="/list\?dir=1">',
    ]

    for pat in patterns:
        if re.search(pat, content, re.IGNORECASE):
            print(f'### Is open directory - {content_url} - matched pattern: {pat}')
            return True
    return False

def function_for_url(regexp_list):
    def get_url_function(f):
        for regexp in regexp_list:
            url_functions.append((re.compile(regexp, flags=re.I | re.U), f))
        return f
    return get_url_function

## url unsafe {}|\^~[]`
## regex no need to escape '!', '"', '%', "'", ',', '/', ':', ';', '<', '=', '>', '@', and "`"
@function_for_url(
    [
        r"^(\/|\.\.\/|\.\/)",
        r"^[0-9\-\./\?=_\&\s%@<>\(\);\+!,\w\$\'–’—”“a°§£Ã¬´c�í¦a]+$",
        r"^[0-9\-\./\?=_\&\s%@<>\(\);\+!,\w\$\'–’—”“a°§£Ã¬´c]*[\?\/][0-9\-\./\?=_\&\s%@<>\(\);\+!,\w\$\'–’—”“a°§£Ã¬:\"¶c´™*]+$",
    ]
)
def relative_url(args):
    out_url = urljoin(args['parent_url'], args['url'])
    parent_host=urlsplit(args['parent_url'])[1]
    db_insert_if_new_url(url=out_url, visited=False, source="relative_url",parent_host=parent_host,db=args['db'])
    return True


@function_for_url(
    [
        r"(\{|\[|\||\}|\]|\~|\^|\\)",
    ]
)
def unsafe_character_url(args):
    return True


@function_for_url(url_all_others_regex)
def do_nothing_url(args):
    # Do nothing with these regex. They are kept here only as a guideline if you
    # want to write your own functions for them
    return True


@function_for_url([r"^https*://", r"^ftp://"])
def full_url(args):
    parent_host=urlsplit(args['parent_url'])[1]
    db_insert_if_new_url(url=args['url'],source="full_url",visited=False,parent_host=parent_host,db=args['db'])
    return True


@function_for_url(
    [
        r"^(mailto:|maillto:|maito:|mail:|malito:|mailton:|\"mailto:|emailto:|maltio:|mainto:|E\-mail:|mailtfo:|mailtp:|mailtop:|mailo:|mail to:|Email para:|email :|email:|E-mail: |mail-to:|maitlo:|mail.to:)"
    ]
)
def email_url(args):
    address_search = re.search(
        r"^(mailto:|maillto:|maito:|mail:|malito:|mailton:|\"mailto:|emailto:|maltio:|mainto:|E\-mail:|mailtfo:|mailtp:|mailtop:|mailo:|mail to:|Email para:|email :|email:|E-mail: |mail-to:|maitlo:|mail.to:)(.*)",
        args['url'],
        flags=re.I | re.U,
    )
    if address_search:
        address = address_search.group(2)
        if re.search(
            r"^([A-Za-z0-9]+[.-_])*[A-Za-z0-9]+@[A-Za-z0-9-]+(\.[A-Z|a-z]{2,})+$",
            address,
        ):
            parent_host=urlsplit(args['parent_url'])[1]
            db_insert_if_new_url(url=args['parent_url'],email=address,source='email_url',parent_host=parent_host,db=args['db'])
            return True
        else:
            return False
    else:
        return False

def get_links(soup, content_url,db):
    #If you want to grep some patterns, use the code below.
    #pattern=r'"file":{".*?":"(.*?)"}'
    #for script in soup.find_all('script',type="text/javascript"):
    #    if re.search(pattern,str(script)):
    #        print(re.search(pattern,str(script))[1])
    tags = soup("a")
    for tag in tags:
        url = tag.get("href", None)

        if type(url) != str:
            continue
        else:
            url = sanitize_url(url)
        found = False
        host=urlsplit(url)[1]
        #the below block ensures that if link takes to a internal directory of the server, it will use the original host
        if host == '':
            host=urlsplit(content_url)[1]
        if not is_host_block_listed(host) and is_host_allow_listed(host) and not is_url_block_listed(url):
            for regex, function in url_functions:
                m = regex.search(url)
                if m:
                    found = True
                    function({'url':url,'parent_url':content_url,'db':db})
                    continue
            if not found:
                out_url = urljoin(content_url, url)
                print("Unexpected URL -{}- Reference URL -{}-".format(url, content_url))
                print("Unexpected URL. Would this work? -{}-".format(out_url))   
                parent_host=urlsplit(content_url)[1]
                if BE_GREEDY:
                    db_insert_if_new_url(url=out_url,source="get_links",visited=False,parent_host=parent_host,db=db)
    return True

def function_for_content_type(regexp_list):
    def get_content_type_function(f):
        for regexp in regexp_list:
            content_type_functions.append((re.compile(regexp, flags=re.I | re.U), f))
        return f
    return get_content_type_function

def get_directory_tree(url):
    #Host will have scheme, hostname and port
    host='://'.join(urlsplit(url)[:2])
    dtree=[]
    for iter in range(1,len(PurePosixPath(unquote(urlparse(url).path)).parts[0:])):
        dtree.append(str(host+'/'+'/'.join(PurePosixPath(unquote(urlparse(url).path)).parts[1:-iter])))
    return dtree

def insert_directory_tree(content_url,db):
    parent_host=urlsplit(content_url)[1]
    for url in get_directory_tree(content_url):
        url = sanitize_url(url)
        db_insert_if_new_url(url=url,words='',content_type='', visited=False,source="insert_directory_tree",parent_host=parent_host,db=db)

@function_for_content_type(content_type_html_regex)
def content_type_download(args):
    try:
        content = args['content']
        # Ensure content is decoded properly
        if isinstance(content, bytes):
            content = content.decode('utf-8', errors='replace')
        soup = BeautifulSoup(content, "html.parser")
    except UnboundLocalError as e:
        print(e)
        db_insert_if_new_url(url=args['url'],content_type=args['content_type'],isopendir=False,visited=True,words='',min_webcontent='',raw_webcontent='',source='content_type_html_regex.exception',parent_host=args['parent_host'],db=args['db'])
        return False
    except bs4.builder.ParserRejectedMarkup as e:
        db_insert_if_new_url(url=args['url'],content_type=args['content_type'],isopendir=False,visited=True,words='',min_webcontent='',raw_webcontent='',source='content_type_html_regex.exception',parent_host=args['parent_host'],db=args['db'])
        print(e)
        return False

    get_links(soup, args['url'],args['db'])
    words = ''
    min_webcontent=''
    raw_webcontent=''
    if EXTRACT_WORDS:
        words = get_words_from_soup(soup)

    if EXTRACT_RAW_WEBCONTENT:
        raw_webcontent=str(soup)[:MAX_WEBCONTENT_SIZE]

    if EXTRACT_MIN_WEBCONTENT:
        min_webcontent=get_min_webcontent(soup)[:MAX_WEBCONTENT_SIZE]

    isopendir = is_open_directory(str(soup), args['url'])
    db_insert_if_new_url(url=args['url'],content_type=args['content_type'],isopendir=isopendir,visited=True,words=words,min_webcontent=min_webcontent,raw_webcontent=raw_webcontent,source='content_type_html_regex',parent_host=args['parent_host'],db=args['db'])
    return True

@function_for_content_type(content_type_plain_text_regex)
def content_type_plain_text(args):
    words = ''
    if EXTRACT_WORDS:
        words = get_words(args['content'])
    db_insert_if_new_url(url=args['url'],content_type=args['content_type'],isopendir=False,visited=True,words=words,source='content_type_plain_text_regex',parent_host=args['parent_host'],db=args['db'])
    return True

@function_for_content_type(content_type_image_regex)
def content_type_images(args):
    global model
    npixels=0
    if CATEGORIZE_NSFW or DOWNLOAD_ALL_IMAGES:
        try:
            img = Image.open(BytesIO(args['content']))
            width, height = img.size
            npixels = width * height
            nsfw_probability=0
            if img.mode == "CMYK":
                img = img.convert("RGB")
            # Check if it's a palette-based image with transparency
            if img.mode == "P" and "transparency" in img.info:
                # Convert to RGBA to handle transparency properly
                img = img.convert("RGBA")
            filename = hashlib.sha512(img.tobytes()).hexdigest() + ".png"
        except UnidentifiedImageError as e:
            #SVG using cairo in the future
            db_insert_if_new_url(url=args['url'], content_type=args['content_type'],source='content_type_images',isopendir=False, visited=True,parent_host=args['parent_host'],resolution=npixels,db=args['db'])
            return False
        except Image.DecompressionBombError as e:
            db_insert_if_new_url(url=args['url'], content_type=args['content_type'],source='content_type_images',isopendir=False, visited=True,parent_host=args['parent_host'],resolution=npixels,db=args['db'])
            return False
        except OSError:
            db_insert_if_new_url(url=args['url'], content_type=args['content_type'],source='content_type_images',isopendir=False, visited=True,parent_host=args['parent_host'],resolution=npixels,db=args['db'])
            return False
        if DOWNLOAD_ALL_IMAGES:
            img.save(IMAGES_FOLDER+'/' + filename, "PNG")
        if CATEGORIZE_NSFW and npixels > MIN_NSFW_RES :
            image = n2.preprocess_image(img, n2.Preprocessing.YAHOO)
            inputs = np.expand_dims(image, axis=0) 
            predictions = model.predict(inputs, verbose=0)
            sfw_probability, nsfw_probability = predictions[0]
            db_insert_if_new_url(args['url'],content_type=args['content_type'],source='content_type_images',visited=True,parent_host=args['parent_host'],isnsfw=nsfw_probability,isopendir=False,resolution=npixels, db=args['db'])
            if nsfw_probability>NSFW_MIN_PROBABILITY:
                print('porn {} {}'.format(nsfw_probability,args['url']))
                if DOWNLOAD_NSFW:
                    img.save(NSFW_FOLDER +'/'+ filename, "PNG")
            else:
                if DOWNLOAD_SFW:
                    img.save(SFW_FOLDER +'/' +filename, "PNG")
    db_insert_if_new_url(url=args['url'], content_type=args['content_type'],source='content_type_images',isopendir=False, visited=True,parent_host=args['parent_host'],resolution=npixels,db=args['db'])
    return True

@function_for_content_type(content_type_midi_regex)
def content_type_midis(args):
    db_insert_if_new_url(
        url=args['url'],
        content_type=args['content_type'],
        isopendir=False,
        visited=True,
        source='content_type_midis',
        parent_host=args['parent_host'],
        db=args['db']
    )
    if not DOWNLOAD_MIDIS:
        return True
    url = args['url']
    base_filename = os.path.basename(urlparse(url).path)
    try:
        decoded_name = unquote(base_filename)
    except Exception:
        decoded_name = base_filename
    # Separate extension (e.g., ".pdf")
    name_part, ext = os.path.splitext(decoded_name)
    # Sanitize both parts
    name_part = re.sub(r"[^\w\-.]", "_", name_part)
    ext = re.sub(r"[^\w\-.]", "_", ext)
    # Create URL hash prefix (always fixed length)
    url_hash = hashlib.sha256(url.encode()).hexdigest()
    # Max length for entire filename (255) minus hash + dash + extension + safety margin
    max_name_length = MAX_FILENAME_LENGTH - len(url_hash) - 1 - len(ext)
    if len(name_part) > max_name_length:
        name_part = name_part[:max_name_length - 3] + "..."
    safe_filename = f"{url_hash}-{name_part}{ext}"
    filepath = os.path.join(MIDIS_FOLDER, safe_filename)
    with open(filepath, "wb") as f:
        f.write(args['content'])
    return True

@function_for_content_type(content_type_audio_regex)
def content_type_audios(args):
    db_insert_if_new_url(
        url=args['url'],
        content_type=args['content_type'],
        isopendir=False,
        visited=True,
        source='content_type_audios',
        parent_host=args['parent_host'],
        db=args['db']
    )
    if not DOWNLOAD_AUDIOS:
        return True
    url = args['url']
    base_filename = os.path.basename(urlparse(url).path)
    try:
        decoded_name = unquote(base_filename)
    except Exception:
        decoded_name = base_filename
    # Separate extension (e.g., ".pdf")
    name_part, ext = os.path.splitext(decoded_name)
    # Sanitize both parts
    name_part = re.sub(r"[^\w\-.]", "_", name_part)
    ext = re.sub(r"[^\w\-.]", "_", ext)
    # Create URL hash prefix (always fixed length)
    url_hash = hashlib.sha256(url.encode()).hexdigest()
    # Max length for entire filename (255) minus hash + dash + extension + safety margin
    max_name_length = MAX_FILENAME_LENGTH - len(url_hash) - 1 - len(ext)
    if len(name_part) > max_name_length:
        name_part = name_part[:max_name_length - 3] + "..."
    safe_filename = f"{url_hash}-{name_part}{ext}"
    filepath = os.path.join(AUDIOS_FOLDER, safe_filename)
    with open(filepath, "wb") as f:
        f.write(args['content'])
    return True

@function_for_content_type(content_type_video_regex)
def content_type_videos(args):
    db_insert_if_new_url(
        url=args['url'],
        content_type=args['content_type'],
        isopendir=False,
        visited=True,
        source='content_type_videos',
        parent_host=args['parent_host'],
        db=args['db']
    )
    if not DOWNLOAD_VIDEOS:
        return True
    url = args['url']
    base_filename = os.path.basename(urlparse(url).path)
    try:
        decoded_name = unquote(base_filename)
    except Exception:
        decoded_name = base_filename
    # Separate extension (e.g., ".pdf")
    name_part, ext = os.path.splitext(decoded_name)
    # Sanitize both parts
    name_part = re.sub(r"[^\w\-.]", "_", name_part)
    ext = re.sub(r"[^\w\-.]", "_", ext)
    # Create URL hash prefix (always fixed length)
    url_hash = hashlib.sha256(url.encode()).hexdigest()
    # Max length for entire filename (255) minus hash + dash + extension + safety margin
    max_name_length = MAX_FILENAME_LENGTH - len(url_hash) - 1 - len(ext)
    if len(name_part) > max_name_length:
        name_part = name_part[:max_name_length - 3] + "..."
    safe_filename = f"{url_hash}-{name_part}{ext}"
    filepath = os.path.join(VIDEOS_FOLDER, safe_filename)
    with open(filepath, "wb") as f:
        f.write(args['content'])
    return True

@function_for_content_type(content_type_pdf_regex)
def content_type_pdfs(args):
    db_insert_if_new_url(
        url=args['url'],
        content_type=args['content_type'],
        isopendir=False,
        visited=True,
        source='content_type_pdfs',
        parent_host=args['parent_host'],
        db=args['db']
    )
    if not DOWNLOAD_PDFS:
        return True
    url = args['url']
    base_filename = os.path.basename(urlparse(url).path)
    try:
        decoded_name = unquote(base_filename)
    except Exception:
        decoded_name = base_filename
    # Separate extension (e.g., ".pdf")
    name_part, ext = os.path.splitext(decoded_name)
    # Sanitize both parts
    name_part = re.sub(r"[^\w\-.]", "_", name_part)
    ext = re.sub(r"[^\w\-.]", "_", ext)
    # Create URL hash prefix (always fixed length)
    url_hash = hashlib.sha256(url.encode()).hexdigest()
    # Max length for entire filename (255) minus hash + dash + extension + safety margin
    max_name_length = MAX_FILENAME_LENGTH - len(url_hash) - 1 - len(ext)
    if len(name_part) > max_name_length:
        name_part = name_part[:max_name_length - 3] + "..."
    safe_filename = f"{url_hash}-{name_part}{ext}"
    filepath = os.path.join(PDFS_FOLDER, safe_filename)
    with open(filepath, "wb") as f:
        f.write(args['content'])
    return True

@function_for_content_type(content_type_doc_regex)
def content_type_docs(args):
    db_insert_if_new_url(
        url=args['url'],
        content_type=args['content_type'],
        isopendir=False,
        visited=True,
        source='content_type_docs',
        parent_host=args['parent_host'],
        db=args['db']
    )
    if not DOWNLOAD_DOCS:
        return True
    url = args['url']
    base_filename = os.path.basename(urlparse(url).path)
    try:
        decoded_name = unquote(base_filename)
    except Exception:
        decoded_name = base_filename
    # Separate extension (e.g., ".pdf")
    name_part, ext = os.path.splitext(decoded_name)
    # Sanitize both parts
    name_part = re.sub(r"[^\w\-.]", "_", name_part)
    ext = re.sub(r"[^\w\-.]", "_", ext)
    # Create URL hash prefix (always fixed length)
    url_hash = hashlib.sha256(url.encode()).hexdigest()
    # Max length for entire filename (255) minus hash + dash + extension + safety margin
    max_name_length = MAX_FILENAME_LENGTH - len(url_hash) - 1 - len(ext)
    if len(name_part) > max_name_length:
        name_part = name_part[:max_name_length - 3] + "..."
    safe_filename = f"{url_hash}-{name_part}{ext}"
    filepath = os.path.join(DOCS_FOLDER, safe_filename)
    with open(filepath, "wb") as f:
        f.write(args['content'])
    return True

@function_for_content_type(content_type_compressed_regex)
def content_type_compresseds(args):
    db_insert_if_new_url(
        url=args['url'],
        content_type=args['content_type'],
        isopendir=False,
        visited=True,
        source='content_type_compresseds',
        parent_host=args['parent_host'],
        db=args['db']
    )
    if not DOWNLOAD_COMPRESSEDS:
        return True
    url = args['url']
    base_filename = os.path.basename(urlparse(url).path)
    try:
        decoded_name = unquote(base_filename)
    except Exception:
        decoded_name = base_filename
    # Separate extension (e.g., ".pdf")
    name_part, ext = os.path.splitext(decoded_name)
    # Sanitize both parts
    name_part = re.sub(r"[^\w\-.]", "_", name_part)
    ext = re.sub(r"[^\w\-.]", "_", ext)
    # Create URL hash prefix (always fixed length)
    url_hash = hashlib.sha256(url.encode()).hexdigest()
    # Max length for entire filename (255) minus hash + dash + extension + safety margin
    max_name_length = MAX_FILENAME_LENGTH - len(url_hash) - 1 - len(ext)
    if len(name_part) > max_name_length:
        name_part = name_part[:max_name_length - 3] + "..."
    safe_filename = f"{url_hash}-{name_part}{ext}"
    filepath = os.path.join(COMPRESSEDS_FOLDER, safe_filename)
    with open(filepath, "wb") as f:
        f.write(args['content'])
    return True

@function_for_content_type(content_type_all_others_regex)
def content_type_ignore(args):
    # We update as visited.
    db_insert_if_new_url(url=args['url'],visited=True,isopendir=False,content_type=args['content_type'],source='content_type_all_others_regex',parent_host=args['parent_host'],db=args['db'])
    return True

def sanitize_content_type(content_type):
    content_type = content_type.strip() 
    content_type = content_type.rstrip()      
    content_type = re.sub(r'^"(.*)"$', r"\1", content_type) # remove surrounding quotes if present
    content_type = re.sub(r'^content-type: (.*)"$', r"\1", content_type) # remove "content-type:" prefix
    content_type = re.sub(r'^content-type:(.*)"$', r"\1", content_type) # remove "content-type:" prefix  
    content_type = re.sub(r'^(.*?);.*$', r"\1",content_type) # keep only the type/subtype part
    content_type = re.sub(r'\s+', '', content_type)  # remove any remaining spaces
    return content_type

def get_page(url, driver, db):
    original_url=url
    driver = read_web(url, driver)  # Fetch the page using Selenium
    parent_host = urlsplit(url)[1]  # Get the parent host from the URL
    if driver:
        for request in driver.requests:
            if request.response:
                # Check if the response status code indicates redirection
                status_code = request.response.status_code
                if status_code in [301, 302, 303, 307, 308]:  # Redirection status codes
                    # Get the new URL from the Location header
                    db_insert_if_new_url(url=url,visited=True,isopendir=False,source='get_page.redirect',parent_host=parent_host,db=db)
                # Continue with normal content processing 
                if 'Content-Type' in request.response.headers:
                    url=request.url
                    host=urlsplit(url)[1]

                    try: 
                        content = decode(request.response.body, request.response.headers.get('Content-Encoding', 'identity'))
                    except ValueError as e:  # 🛠 Catch specific Brotli decompression failure
                        if "BrotliDecompress failed" in str(e):
                            db_insert_if_new_url(url=url,visited=True,source='BrotliDecompressFailed',parent_host=parent_host,db=db)
                            continue
                        elif "LookupError when decoding" in str(e):
                            db_insert_if_new_url(url=url,visited=True,source='lookuperror',parent_host=parent_host,db=db)
                            continue
                        elif "EOFError when decoding" in str(e):
                            db_insert_if_new_url(url=url,visited=True,source='EOFERROR',parent_host=parent_host,db=db)
                            continue
                        elif "BadGzipFile when decoding" in str(e):
                            db_insert_if_new_url(url=url,visited=True,source='BadGzipFile',parent_host=parent_host,db=db)
                            continue
                        elif "UnicodeDecodeError when decoding" in str(e):
                            db_insert_if_new_url(url=url,visited=True,source='UnicodeDecodeError',parent_host=parent_host,db=db)
                            continue
                        else:
                            print(f"\033[91m🚨 !!!! This was not updated in the database, you need to deal with this error in the code function get_page [DECODE ERROR] {url} - {e} -\033[0m")
                            continue
                    content_type = request.response.headers['Content-Type']
                    content_type = sanitize_content_type(content_type)
                    host = urlsplit(url)[1]  # Extract host from the URL

                    if not is_host_block_listed(host) and is_host_allow_listed(host) and not is_url_block_listed(url):
                        if HUNT_OPEN_DIRECTORIES:
                            insert_directory_tree(url, db)

                        found = False
                        for regex, function in content_type_functions:
                            m = regex.search(content_type)
                            if m:
                                found = True
                                function({'url': url, 'visited': True, 'content_type': content_type, 
                                          'content': content, 'source': 'get_page', 'words': '', 
                                          'parent_host': parent_host, 'db': db})
                        if not found:
                            print(f"UNKNOWN type -{url}- -{content_type}-")
        #force update on main url
        db_insert_if_new_url(url=url,visited=True,source='get_page.end',parent_host=parent_host,db=db)
    #force update on main url
    db_insert_if_new_url(url=original_url,visited=True,source='get_page.end.original',parent_host=parent_host,db=db)


def break_after(seconds=60):
    def timeout_handler(signum, frame):  # Custom signal handler
        raise TimeoutException
    def function(function):
        def wrapper(*args, **kwargs):
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(seconds)
            try:
                res = function(*args, **kwargs)
                signal.alarm(0)  # Clear alarm
                return res
            except TimeoutException:
                print(
                    "Oops, timeout: {} {} {} {} sec reached.".format(
                        seconds, function.__name__, args, kwargs
                    )
                )
            return
        return wrapper
    return function

## This "break_after" is a decorator, not intended for timeouts,
## but for links that take too long downloading, like streamings
## or large files.
@break_after(MAX_DOWNLOAD_TIME)
def read_web(url,driver):
    try:
        if url.startswith('http://'):
            url=HTTPS_EMBED+url
        driver.get(url)
        return driver
    except Exception as e:
        print(e)
        return False

class TimeoutException(Exception):  # Custom exception class
    pass

def initialize_driver():
    user_agent = UserAgent().random
    options = webdriver.ChromeOptions()
    options.add_argument(f'user-agent={user_agent}')
    prefs = {"download.default_directory": DIRECT_LINK_DOWNLOAD_FOLDER,}
    if not CATEGORIZE_NSFW and not DOWNLOAD_ALL_IMAGES and not FORCE_IMAGE_LOAD:
        prefs["profile.managed_default_content_settings.images"] = 2  # disable images
    if BLOCK_CSS:
        prefs["profile.managed_default_content_settings.stylesheets"] = 2  # disable CSS
        prefs["profile.managed_default_content_settings.fonts"] = 2  # disable CSS
    options.add_experimental_option("prefs", prefs)
    if PERFORMANCE_OPTIMIZED:
        options.add_argument('--disable-extensions')
        options.add_argument('--disable-gpu')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-webgl')
        options.add_argument('--blink-settings=imagesEnabled=false')
    options.add_argument('--ignore-certificate-errors-spki-list')
    options.add_argument('--ignore-ssl-errors')
    options.add_argument("--ignore-certificate-errors")
    options.add_argument("--disable-web-security")
    options.add_argument("--allow-running-insecure-content")
    #options.add_argument('--disable-webrtc')
    #options.add_argument('--disable-geolocation')
    #options.add_argument('--disable-infobars')
    #options.add_argument('--disable-popup-blocking')
    #options.add_argument('--disable-javascript')
    #options.add_argument('--proxy-server=http://your-proxy-server:port')
    #options.add_argument('--proxy-server=http://'+PROXY_HOST+':'PROXY_PORT)
    driver = webdriver.Chrome(options=options)
    driver.set_window_size(SELENIUM_WIDTH, SELENIUM_HEIGHT)
    return driver

def crawler(db):
    for iteration in range(ITERATIONS):
        driver = initialize_driver()
        random_urls = get_random_unvisited_domains(db=db)
        for target_url in random_urls:
            if (
                not is_host_block_listed(target_url['host']) and
                is_host_allow_listed(target_url['host']) and
                not is_url_block_listed(target_url['url'])
            ):
                try:
                    print('    {}'.format(target_url['url']))
                    del driver.requests
                    get_page(target_url['url'], driver,db)
                    if HUNT_OPEN_DIRECTORIES:
                        insert_directory_tree(target_url['url'],db)
                except UnicodeEncodeError:
                    pass
        driver.quit()

def get_random_unvisited_domains(db, size=RANDOM_SITES_QUEUE):
    """
    Randomly selects between different spreading strategies based on weighted probabilities.

    Args:
        db: Database connection
        size: Number of domains to retrieve
    Returns:
        List of domains selected by the chosen method
    """
    # Default weights if none provided
    if METHOD_WEIGHTS is None:
        method_weights = {
            "from_file":    0,
            "fewest_urls":  1,
            "less_visited": 2,
            "oldest":       2,
            "host_prefix":  2,
            "random":       1
        }
    else:
        method_weights=METHOD_WEIGHTS

    # Filter out methods with zero weight
    active_methods = {name: weight for name, weight in method_weights.items() if weight > 0}

    # If no methods have weights > 0, return empty list
    if not active_methods:
        print("No active methods configured (all weights are 0)")
        return []

    # Normalize weights to sum to 1.0
    total_weight = sum(active_methods.values())
    normalized_weights = {name: weight/total_weight for name, weight in active_methods.items()}

    # Set up method mapping
    method_functions = {
        "from_file": lambda: get_url_from_file(),
        "fewest_urls": lambda: get_least_covered_random_hosts(db, size=size),
        "less_visited": lambda: get_urls_from_least_visited_hosts(db, size=size),
        "oldest": lambda: get_oldest_unvisited_urls_from_bucket(db, size=size),
        "host_prefix": lambda: get_urls_by_random_bucket_and_host_prefix(db, size=size),
        "random": lambda: get_random_host_domains(db, size=size)
    }

    try:
        # Choose method based on normalized weights
        methods = list(normalized_weights.keys())
        weights = list(normalized_weights.values())
        chosen_method = random.choices(methods, weights=weights, k=1)[0]
        print(f'Selected method: \033[32m{chosen_method}\033[0m')
        return method_functions[chosen_method]()

    except NotFoundError as e:
        if "index_not_found_exception" in str(e):
            print("Elasticsearch index missing. Creating now...")
            db_create_database(INITIAL_URL, db=db)
            return []
        return []
    except RequestError as e:
        print("Elasticsearch request error:", e)
        return []
    except Exception as e:
        print(f"Unhandled error in get_random_unvisited_domains: {e}")
        return []

def get_url_from_file():
    with open(URL_FILE, 'r', encoding='utf-8') as file:
        urls = []
        for line in file:
            # Strip whitespace and skip empty lines
            url = line.strip()
            if not url:
                continue
            # Extract hostname from the URL
            parsed_url = urlparse(url)
            host = parsed_url.netloc
            urls.append({"url": url, "host": host})
        #random.shuffle(urls)
        return urls

def get_least_covered_random_hosts(db, size=100):
    """Returns 'size' hosts from a random bucket with the fewest unvisited URLs, and one random URL per host."""
    for attempt in range(MAX_ES_RETRIES):
        random_bucket = random.randint(0, ELASTICSEARCH_RANDOM_BUCKETS - 1)
        print(f'    Selected bucket: \033[33m{random_bucket}\033[0m')

        # Step 1: Get hosts with fewest unvisited URLs
        agg_query = {
            "size": 0,
            "query": {
                "bool": {
                    "must": [
                        {"term": {"random_bucket": random_bucket}},
                        {
                            "bool": {
                                "should": [
                                    {"term": {"visited": False}},
                                    {"bool": {"must_not": {"exists": {"field": "visited"}}}}
                                ],
                                "minimum_should_match": 1
                            }
                        }
                    ]
                }
            },
            "aggs": {
                "hosts": {
                    "terms": {
                        "field": "host",
                        "size": size,
                        "order": {"_count": "asc"}  # Fewest unvisited URLs first
                    }
                }
            }
        }

        agg_response = db.con.search(index=URLS_INDEX, body=agg_query)
        buckets = agg_response.get("aggregations", {}).get("hosts", {}).get("buckets", [])
        hosts = [bucket["key"] for bucket in buckets]
        for bucket in buckets:
            print('    \033[35m{} \t {}\033[0m'.format(bucket['doc_count'],bucket['key']))

        if not hosts:
            continue

        # Step 2: For each host, get one random unvisited URL
        results = []
        for host in hosts:
            query = {
                "size": 1,
                "query": {
                    "bool": {
                        "must": [
                            {"term": {"host": host}},
                            {"term": {"random_bucket": random_bucket}},
                            {
                                "bool": {
                                    "should": [
                                        {"term": {"visited": False}},
                                        {"bool": {"must_not": {"exists": {"field": "visited"}}}}
                                    ],
                                    "minimum_should_match": 1
                                }
                            }
                        ]
                    }
                },
                "sort": [
                    {
                        "_script": {
                            "type": "number",
                            "script": {
                                "lang": "painless",
                                "source": "Math.random()"
                            },
                            "order": "asc"
                        }
                    }
                ]
            }

            response = db.con.search(index=URLS_INDEX, body=query)
            hits = response.get("hits", {}).get("hits", [])
            if hits:
                results.append({
                    "url": hits[0]["_source"]["url"],
                    "host": host
                })

        if results:
            random.shuffle(results)
            return results

    return []

def get_urls_from_least_visited_hosts(db, size=100):
    """Fetch 1 truly unvisited URL per host from a random bucket."""
    for attempt in range(MAX_ES_RETRIES):
        random_bucket = random.randint(0, ELASTICSEARCH_RANDOM_BUCKETS - 1)
        print(f'    Selected bucket: \033[33m{random_bucket}\033[0m')

        query_body = {
            "size": size,
            "query": {
                "bool": {
                    "must": [
                        {"term": {"random_bucket": random_bucket}},
                        {
                            "bool": {
                                "should": [
                                    {"term": {"visited": False}},
                                    {"bool": {"must_not": {"exists": {"field": "visited"}}}}
                                ],
                                "minimum_should_match": 1
                            }
                        }
                    ]
                }
            },
            "collapse": {
                "field": "host",
                "inner_hits": {
                    "name": "least_visited_hit",
                    "size": 1,
                    "sort": [
                        {
                            "_script": {
                                "type": "number",
                                "script": {
                                    "lang": "painless",
                                    "source": "Math.random()"
                                },
                                "order": "asc"
                            }
                        }
                    ]
                }
            },
            "_source": ["host"]
        }

        response = db.con.search(index=URLS_INDEX, body=query_body)
        results = response.get('hits', {}).get('hits', [])
        for result in results:
            print('    \033[35m{} \t {}\033[0m'.format(result['_score'],result['_source']['host']))
        if results:
            random.shuffle(results)
            return [{
                "url": r["inner_hits"]["least_visited_hit"]["hits"]["hits"][0]["_source"]["url"],
                "host": r["_source"]["host"]
            } for r in results]

    return []


def get_oldest_unvisited_urls_from_bucket(db, size=100):
    """Get the oldest unvisited URLs from a random bucket using created_at timestamp."""
    for attempt in range(MAX_ES_RETRIES):
        random_bucket = random.randint(0, ELASTICSEARCH_RANDOM_BUCKETS - 1)
        print(f'    Selected bucket: \033[33m{random_bucket}\033[0m')

        query_body = {
            "size": size,
            "query": {
                "bool": {
                    "must": [
                        {"term": {"random_bucket": random_bucket}},
                        {
                            "bool": {
                                "should": [
                                    {"term": {"visited": False}},
                                    {"bool": {"must_not": {"exists": {"field": "visited"}}}}
                                ],
                                "minimum_should_match": 1
                            }
                        }
                    ]
                }
            },
            "sort": [
                { "created_at": { "order": "asc" } }
            ]
        }

        response = db.con.search(index=URLS_INDEX, body=query_body)
        hits = response.get('hits', {}).get('hits', [])
        if hits:
            for hit in hits:
                print(f'    \033[35m{hit["_source"]["url"]}\033[0m')
            random.shuffle(hits)  # Shuffle the list in-place
            return [{
                "url": hit["_source"]["url"],
                "host": hit["_source"]["host"]
            } for hit in hits]

    return []

def get_urls_by_random_bucket_and_host_prefix(db, size=100):
    """Get 1 unvisited URL per host from a random bucket where host starts with a random character."""
    for attempt in range(MAX_ES_RETRIES):
        random_bucket = random.randint(0, ELASTICSEARCH_RANDOM_BUCKETS - 1)
        print(f'    Selected bucket: \033[33m{random_bucket}\033[0m')
        prefix_char = random.choice(string.ascii_lowercase + string.digits)

        query_body = {
            "size": size,
            "query": {
                "bool": {
                    "must": [
                        {"term": {"random_bucket": random_bucket}},
                        {"prefix": {"host": prefix_char}},
                        {
                            "bool": {
                                "should": [
                                    {"term": {"visited": False}},
                                    {"bool": {"must_not": {"exists": {"field": "visited"}}}}
                                ],
                                "minimum_should_match": 1
                            }
                        }
                    ]
                }
            },
            "collapse": {
                "field": "host",
                "inner_hits": {
                    "name": "random_unvisited_url",
                    "size": 1,
                    "sort": [
                        {
                            "_script": {
                                "type": "number",
                                "script": {
                                    "lang": "painless",
                                    "source": "Math.random()"
                                },
                                "order": "asc"
                            }
                        }
                    ]
                }
            },
            "_source": ["host"]
        }
        response = db.con.search(index=URLS_INDEX, body=query_body)
        results = response.get('hits', {}).get('hits', [])
        if results:
            urls = [{
                "url": r["inner_hits"]["random_unvisited_url"]["hits"]["hits"][0]["_source"]["url"],
                "host": r["_source"]["host"]
            } for r in results]
            for url in urls:
                print('    \033[35m{}\033[0m'.format(url['url']))
            random.shuffle(urls)
            return urls
    return []

def get_random_host_domains(db, size=100):
    for attempt in range(MAX_ES_RETRIES):
        random_bucket = random.randint(0, ELASTICSEARCH_RANDOM_BUCKETS - 1)
        print(f'    Selected bucket: \033[33m{random_bucket}\033[0m')
        query_body = {
            "size": size,
            "query": {
                "bool": {
                    "must": [
                        {"term": {"random_bucket": random_bucket}}
                    ],
                    "should": [
                        {"term": {"visited": False}},
                        {"bool": {"must_not": {"exists": {"field": "visited"}}}}
                    ],
                    "minimum_should_match": 1
                }
            },
            "collapse": {
                "field": "host",
                "inner_hits": {
                    "name": "random_hit",
                    "size": 1,
                    "sort": [
                        {
                            "_script": {
                                "type": "number",
                                "script": {
                                    "lang": "painless",
                                    "source": "Math.random()"
                                },
                                "order": "asc"
                            }
                        }
                    ]
                }
            }
        }

        response = db.con.search(index=URLS_INDEX, body=query_body)
        results = response.get('hits', {}).get('hits', [])

        if results:
            random.shuffle(results)
            result = [{
                "url": r["inner_hits"]["random_hit"]["hits"]["hits"][0]["_source"]["url"],
                "host": r["_source"]["host"]
            } for r in results]
            for url in result:
                print('    \033[35m{}\033[0m'.format(url['url']))
            return result
        return []

def db_create_database(initial_url, db):
    print("Creating Elasticsearch index structure.")
    now_iso = datetime.now(timezone.utc).isoformat()
    # Define mappings for the URLS_INDEX
    urls_mapping = {
        "mappings": {
            "properties": {
                "url": {"type": "keyword"},
                "visited": {"type": "boolean"},
                "isopendir": {"type": "boolean"},
                "isnsfw": {"type": "float"},
                "content_type": {"type": "keyword"},
                "source": {"type": "keyword"},
                "words": {"type": "keyword"},
                "host": {"type": "keyword"},
                "parent_host": {"type": "keyword"},
                "host_levels": {"type": "keyword"},
                "directory_levels": {"type": "keyword"},
                "host_levels": {"type": "keyword"},
                "file_extension": {"type": "keyword"},
                "has_query": {"type": "boolean"},
                "query_variables": {"type": "keyword"},
                "query_values": {"type": "keyword"},
                **{
                    f"directory_level_{i+1}": {"type": "keyword"}
                    for i in range(MAX_DIR_LEVELS)
                },
                **{
                    f"host_level_{i+1}": {"type": "keyword"}
                    for i in range(MAX_HOST_LEVELS)
                },
                "emails": {"type": "keyword"},
                "resolution": {"type": "integer"},
                "random_bucket": {"type": "integer"},
                "created_at": {"type": "date"},
                "updated_at": {"type": "date"},
                "opendir_category": {"type": "keyword"},
                "min_webcontent": {"type": "text"},
                "raw_webcontent": {"type": "text"}
            }
        }
    }

    try:
        if not db.con.indices.exists(index=URLS_INDEX):
            db.con.indices.create(index=URLS_INDEX, body=urls_mapping)
            print("Created {} index.".format(URLS_INDEX))
            db_insert_if_new_url(
                    url=initial_url,
                    source='db_create_database',
                    parent_host=urlsplit(initial_url)[1],
                    db=db
                )
            print("Inserted initial url {}.".format(initial_url))
        return True
    except Exception as e:
        print("Error creating indices or inserting initial document:", e)
        return False

def fast_extension_crawler(url, extension, content_type_patterns, db):
    headers = {"User-Agent": UserAgent().random}
    try:
        head_resp = requests.head(url, timeout=(10, 10), allow_redirects=True, verify=False, headers=headers)
    except Exception as e:
        db_insert_if_new_url(url=url, visited=True, words='', min_webcontent='', raw_webcontent='',
                             source='fast_extension_crawler.head.exception', db=db)
        return

    if (400 <= head_resp.status_code < 500):
        db_insert_if_new_url(url=url, visited=True, words='', min_webcontent='', raw_webcontent='', source='fast_extension_crawler.head.status_code_4xx', db=db)
    elif not (200 <= head_resp.status_code < 300):
        return
    
    print('url {} {}'.format(head_resp.status_code, url))
    content_type = head_resp.headers.get("Content-Type", "").lower().split(";")[0].strip()
    if not any(re.match(pattern, content_type) for pattern in content_type_patterns):
        return

    try:
        host = urlparse(url).hostname or ""
        if is_host_block_listed(host) or not is_host_allow_listed(host) or is_url_block_listed(url):
            return

        if HUNT_OPEN_DIRECTORIES:
            insert_directory_tree(url, db)

        found = False
        for regex, function in content_type_functions:
            if regex.search(content_type):
                found = True

                needs_download = (
                    (function.__name__ == "content_type_docs" and DOWNLOAD_DOCS) or
                    (function.__name__ == "content_type_pdfs" and DOWNLOAD_PDFS) or
                    (function.__name__ == "content_type_compresseds" and DOWNLOAD_COMPRESSEDS) or
                    (function.__name__ == "content_type_audios" and DOWNLOAD_AUDIOS) or
                    (function.__name__ == "content_type_midis" and DOWNLOAD_MIDIS) or
                    (function.__name__ == "content_type_images" and DOWNLOAD_NSFW) or 
                    (function.__name__ == "content_type_images" and DOWNLOAD_SFW) or 
                    (function.__name__ == "content_type_images" and DOWNLOAD_ALL_IMAGES)
                )

                content = None
                if needs_download:
                    try:
                        get_resp = requests.get(url, timeout=(10, 30), stream=True, allow_redirects=True, verify=False, headers=headers)
                        content = get_resp.content
                    except Exception as e:
                        print(f"[FAST CRAWLER] Failed GET for {url}: {e}")
                        return

                function({
                    'url': url,
                    'visited': True,
                    'content_type': content_type,
                    'content': content,
                    'source': 'fast_extension_crawler',
                    'words': '',
                    'parent_host': '',
                    'db': db
                })
                break

        if not found:
            print(f"[FAST CRAWLER] UNKNOWN type -{url}- -{content_type}-")

    except Exception as e:
        print(f"[FAST CRAWLER] Error processing {url}: {e}")
    time.sleep(random.uniform(FAST_RANDOM_MIN_WAIT,FAST_RANDOM_MAX_WAIT))

def run_fast_extension_pass(db, max_workers=MAX_FAST_WORKERS):
    shuffled_extensions = list(EXTENSION_MAP.items())
    random.shuffle(shuffled_extensions)

    for extension, content_type_patterns in shuffled_extensions:
        buckets = list(range(ELASTICSEARCH_RANDOM_BUCKETS))
        random.shuffle(buckets)

        for random_bucket in buckets:
            print(f"[FAST CRAWLER] Extension: {extension} | Bucket: \033[33m{random_bucket}\033[0m")

            query = {
                "bool": {
                    "must": [
                        {"term": {"visited": False}},
                        {"wildcard": {"url": f"*{extension}"}},
                        {"term": {"random_bucket": random_bucket}}
                    ]
                }
            }

            try:
                result = db.es.search(index=URLS_INDEX, query=query, size=10000)
                urls = result.get("hits", {}).get("hits", [])

                if not urls:
                    continue

                random.shuffle(urls)

                # Prepare (url, extension) tuples
                urls_with_extensions = [
                    (hit["_source"]["url"], extension)
                    for hit in urls
                ]

                # Run fast crawlers concurrently
                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    futures = [
                        executor.submit(fast_extension_crawler, url, extension, content_type_patterns, db)
                        for url, extension in urls_with_extensions
                    ]
                    for future in as_completed(futures):
                        try:
                            future.result()
                        except Exception as e:
                            print(f"[FAST CRAWLER] Exception during execution: {e}")

            except Exception as e:
                print(f"[FAST CRAWLER] Error retrieving URLs for extension {extension} in bucket {random_bucket}: {e}")

#def is_valid_url(url: str) -> bool:
#    try:
#        parsed = urlparse(url)
#        return parsed.scheme in {"http", "https", "ftp"} and bool(parsed.netloc)
#    except Exception:
#        return False

def remove_invalid_urls(db):
    """
    Deletes documents from Elasticsearch where the 'url' field is invalid, based on urlparse.
    """
    deleted = 0
    query = {"query": {"match_all": {}}}

    for doc in helpers.scan(db.es, index=URLS_INDEX, query=query):
        url = doc['_source'].get('url')
        if not url:
            continue
        pre_url = url
        url=sanitize_url(url)
        if pre_url != url :
            print(f"🧹 Deleted sanitized URL: -{pre_url}- inserting -{url}-")
            db_insert_if_new_url(url=url, visited=False, source="remove_invalid_urls",db=db)
            #db.es.delete(index=URLS_INDEX, id=doc['_id'])
            deleted += 1
        #if not is_valid_url(url):
        #    print(f"Sanitized url continues to be invalid!!!!: {url}")
        #    #db.es.delete(index=URLS_INDEX, id=doc['_id'])
        #    deleted += 1
    print(f"\n✅ Done. Total invalid URLs deleted: {deleted}")

def remove_blocked_hosts_from_es_db(db):
    compiled_blocklist = [re.compile(pattern) for pattern in host_regex_block_list]
    def is_blocked(host):
        return any(regex.search(host) for regex in compiled_blocklist)
    deleted = 0
    query = {"query": {"match_all": {}}}
    try:
        for doc in helpers.scan(db.es, index=URLS_INDEX, query=query):
            url = doc['_source'].get('url')
            if not url:
                continue
            host = urlsplit(url).hostname or ''
            if is_blocked(host):
                db.es.delete(index=URLS_INDEX, id=doc['_id'])
                print(f"🧹 Deleted: {url}")
                deleted += 1
    except NotFoundError as e:
        if "index_not_found_exception" in str(e):
            print("Elasticsearch index missing. Creating now...")
            db_create_database(INITIAL_URL, db=db)
    print(f"\n✅ Done. Total deleted: {deleted}")

def make_https_app():
    return web.Application([
        (r"/(.*)", web.StaticFileHandler, {
            "path": os.getcwd(),
            "default_filename": "index.html"
        })
    ], debug=False)

def start_https_server():
    # Generate self-signed cert (only if not exists)
    if not (os.path.exists("cert.pem") and os.path.exists("key.pem")):
        os.system("openssl req -nodes -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -days 365 -subj '/CN=mylocalhost'")

    app = make_https_app()
    server = httpserver.HTTPServer(app, ssl_options={
        "certfile": "cert.pem",
        "keyfile": "key.pem",
    })
    server.listen(EMBED_PORT)
    print(f"Serving HTTPS at https://localhost:{EMBED_PORT}")

    # Run the server in the background using Tornado's IOLoop
    ioloop.IOLoop.current().spawn_callback(lambda: None)  # ensure IOLoop starts
    ioloop.IOLoop.current().start()


def get_instance_number():
    """
    Uses file locking to detect which instance this is.
    Returns 1 for the first, 2 for the second, 3+ for others.
    Keeps the lock file open to avoid lock release.
    """
    global lock_file
    try:
        os.makedirs("/tmp/instance_flags", exist_ok=True)
        for i in range(1, 100):
            lock_path = f"/tmp/instance_flags/instance_{i}.lock"
            lock_file = open(lock_path, "w")  # keep open!
            try:
                fcntl.flock(lock_file, fcntl.LOCK_EX | fcntl.LOCK_NB)
                return i
            except BlockingIOError:
                lock_file.close()
                continue
    except Exception as e:
        print(f"Error determining instance number: {e}")
    return 999

def main():
    instance = get_instance_number()
    db = DatabaseConnection()
    if instance == 1:
        print("Instance 1: Running HTTPS webserver to embed http site for compatibility with selenium-wire.")
        # Run HTTPS in a background thread to avoid blocking the crawler
        import threading
        threading.Thread(target=start_https_server, daemon=True).start()
        time.sleep(1)  # Give HTTPS server a head start
        print("Instance 1: And now some housekeeping. Removing urls from hosts that are blocklisted and should be removed.")
        remove_blocked_hosts_from_es_db(db)
        if LOOK_FOR_INVALID_URLS:
            print("Instance 1: Deleting invalid urls.")
            remove_invalid_urls(db)
        print("Instance 1: Let's go full crawler mode.")
        crawler(db)
    elif instance == 2:
        print("Instance 2: Running fast extension pass only. Not everything needs selenium... running requests in urls that looks like files.")
        run_fast_extension_pass(db)
    elif instance == 3:
        print("Instance 3: Scanning IPs in some unconventional ports and protocols combinations.")
        try:
            subprocess.run(["venv/bin/python3", "scanner.py"], check=True)
        except subprocess.CalledProcessError as e:
            print(f"Error while running scanner.py: {e}")
    else:
        print(f"Instance {instance}: Running full crawler.")
        crawler(db)
    db.close()

if __name__ == "__main__":
    main()
